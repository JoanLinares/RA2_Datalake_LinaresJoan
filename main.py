#!/usr/bin/env python3
"""
Main - Orquestador del Pipeline de ETL
Fase 1: Extracci√≥n de datos de Polymarket a Delta Lake
Fase 2: Transformaci√≥n, Carga y Validaci√≥n en NeonDB

Flujo autom√°tico:
1. Si no existe datalake/raw ‚Üí ejecuta extractor_polymarket
2. Ejecuta DataTransformer (normalizaci√≥n de datos)
3. Ejecuta WarehouseValidator (validaci√≥n de integridad)
4. Ejecuta WarehouseLoader (carga a NeonDB)
"""
import os
import sys
import subprocess
import logging
from pathlib import Path
from dotenv import load_dotenv

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv()


def check_datalake_exists() -> bool:
    """Verifica si la carpeta datalake/raw existe con datos"""
    datalake_path = Path("datalake/raw")
    
    if not datalake_path.exists():
        logger.warning("‚ùå Carpeta datalake/raw no encontrada")
        return False
    
    # Verificar que existan al menos algunas subcarpetas
    expected_folders = ["events", "markets", "series", "tags"]
    folders_found = [
        (datalake_path / folder).exists() 
        for folder in expected_folders
    ]
    
    if not any(folders_found):
        logger.warning("‚ùå Subcarpetas de datalake/raw no encontradas")
        return False
    
    logger.info("‚úÖ Carpeta datalake/raw encontrada")
    return True


def run_extractor():
    """Ejecuta el extractor de Polymarket"""
    logger.info("\n" + "="*70)
    logger.info("FASE 1: EXTRACCI√ìN DE DATOS DE POLYMARKET")
    logger.info("="*70)
    
    try:
        result = subprocess.run(
            [sys.executable, "extractor_polymarket.py"],
            cwd=Path(__file__).parent,
            capture_output=False
        )
        
        if result.returncode != 0:
            logger.error("‚ùå Error durante la extracci√≥n")
            return False
        
        logger.info("‚úÖ Extracci√≥n completada")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error ejecutando extractor: {e}")
        return False


def run_transformer():
    """Ejecuta el transformador de datos"""
    logger.info("\n" + "="*70)
    logger.info("FASE 2A: TRANSFORMACI√ìN DE DATOS")
    logger.info("="*70)
    
    try:
        from src.utils.transformer_data import DataTransformer
        from deltalake import DeltaTable
        import pandas as pd
        
        datalake_path = Path("datalake/raw")
        
        # Leer y transformar eventos
        logger.info("\n[1/4] Transformando eventos...")
        events_path = datalake_path / "events"
        if events_path.exists():
            try:
                events_df = DeltaTable(str(events_path)).to_pandas()
                logger.info(f"Le√≠dos {len(events_df)} eventos")
                events_df = DataTransformer.validate_and_clean_events(events_df)
                logger.info(f"‚úì Evento limpiados: {len(events_df)} registros")
            except Exception as e:
                logger.warning(f"‚ö† Error transformando eventos: {e}")
        
        # Leer y transformar mercados
        logger.info("\n[2/4] Transformando mercados...")
        markets_path = datalake_path / "markets"
        if markets_path.exists():
            try:
                markets_df = DeltaTable(str(markets_path)).to_pandas()
                logger.info(f"Le√≠dos {len(markets_df)} mercados")
                markets_df = DataTransformer.validate_and_clean_markets(markets_df)
                logger.info(f"‚úì Mercados limpios: {len(markets_df)} registros")
            except Exception as e:
                logger.warning(f"‚ö† Error transformando mercados: {e}")
        
        # Leer series
        logger.info("\n[3/4] Leyendo series...")
        series_path = datalake_path / "series"
        if series_path.exists():
            try:
                series_df = DeltaTable(str(series_path)).to_pandas()
                logger.info(f"‚úì Series le√≠das: {len(series_df)} registros")
            except Exception as e:
                logger.warning(f"‚ö† Error leyendo series: {e}")
        
        # Leer tags
        logger.info("\n[4/4] Leyendo tags...")
        tags_path = datalake_path / "tags"
        if tags_path.exists():
            try:
                tags_df = DeltaTable(str(tags_path)).to_pandas()
                logger.info(f"‚úì Tags le√≠das: {len(tags_df)} registros")
            except Exception as e:
                logger.warning(f"‚ö† Error leyendo tags: {e}")
        
        logger.info("‚úÖ Transformaci√≥n completada")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error durante transformaci√≥n: {e}")
        return False


def run_validator():
    """Ejecuta la validaci√≥n de warehouse"""
    logger.info("\n" + "="*70)
    logger.info("FASE 2B: VALIDACI√ìN PRE-CARGA")
    logger.info("="*70)
    
    try:
        from src.utils.validator_warehouse import WarehouseValidator
        
        DATABASE_URL = os.getenv('DATABASE_URL')
        if not DATABASE_URL:
            logger.error("‚ùå DATABASE_URL no encontrada en .env")
            return False
        
        validator = WarehouseValidator(DATABASE_URL)
        validator.connect()
        
        # Validar archivos de Delta Lake
        logger.info("\nüìÅ Verificando disponibilidad de datos...")
        datalake_path = Path("datalake/raw")
        
        expected_items = {
            "events": datalake_path / "events",
            "markets": datalake_path / "markets",
            "series": datalake_path / "series",
            "tags": datalake_path / "tags"
        }
        
        all_exist = True
        for name, path in expected_items.items():
            if path.exists():
                logger.info(f"‚úì {name}: disponible")
            else:
                logger.warning(f"‚úó {name}: no disponible")
                all_exist = False
        
        if not all_exist:
            logger.warning("‚ö† Algunos datos est√°n faltando")
        
        validator.close()
        logger.info("‚úÖ Validaci√≥n completada")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error durante validaci√≥n: {e}")
        return False


def run_loader():
    """Ejecuta el cargador de warehouse"""
    logger.info("\n" + "="*70)
    logger.info("FASE 2C: CARGA EN NEONDB")
    logger.info("="*70)
    
    try:
        from src.warehouse.loader_NeonDB import WarehouseLoader
        
        DATABASE_URL = os.getenv('DATABASE_URL')
        if not DATABASE_URL:
            logger.error("‚ùå DATABASE_URL no encontrada en .env")
            return False
        
        loader = WarehouseLoader(DATABASE_URL)
        loader.connect()
        loader.load_all()
        
        logger.info("‚úÖ Carga completada")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error durante carga: {e}")
        logger.exception("Detalles:")
        return False


def main():
    """Funci√≥n principal - orquesta todo el pipeline"""
    
    logger.info("\n" + "üöÄ "*35)
    logger.info("INICIANDO PIPELINE COMPLETO DE ETL")
    logger.info("Polymarket ‚Üí Delta Lake ‚Üí NeonDB")
    logger.info("üöÄ "*35 + "\n")
    
    # Fase 1: Extracci√≥n
    if not check_datalake_exists():
        logger.info("\n‚öôÔ∏è  Iniciando extracci√≥n de datos...")
        if not run_extractor():
            logger.error("‚ùå Pipeline abortado: fall√≥ extracci√≥n")
            return 1
    else:
        logger.info("‚è≠Ô∏è  Saltando extracci√≥n: datalake/raw ya existe")
    
    # Fase 2A: Transformaci√≥n
    logger.info("\n‚öôÔ∏è  Iniciando transformaci√≥n de datos...")
    if not run_transformer():
        logger.error("‚ùå Pipeline abortado: fall√≥ transformaci√≥n")
        return 1
    
    # Fase 2B: Validaci√≥n
    logger.info("\n‚öôÔ∏è  Iniciando validaci√≥n...")
    if not run_validator():
        logger.error("‚ö†Ô∏è  Advertencia durante validaci√≥n, continuando...")
    
    # Fase 2C: Carga
    logger.info("\n‚öôÔ∏è  Iniciando carga en NeonDB...")
    if not run_loader():
        logger.error("‚ùå Pipeline abortado: fall√≥ carga")
        return 1
    
    # √âxito
    logger.info("\n" + "‚úÖ "*35)
    logger.info("PIPELINE COMPLETADO EXITOSAMENTE")
    logger.info("‚úÖ "*35 + "\n")
    
    logger.info("üìä Resultados:")
    logger.info("  ‚Ä¢ Datos extra√≠dos desde Polymarket API")
    logger.info("  ‚Ä¢ Almacenados en Delta Lake (datalake/raw/)")
    logger.info("  ‚Ä¢ Transformados y normalizados")
    logger.info("  ‚Ä¢ Cargados en PostgreSQL (NeonDB)")
    logger.info("  ‚Ä¢ Validaci√≥n de integridad completada")
    logger.info("\nPr√≥ximos pasos:")
    logger.info("  ‚Üí Explorar datos en NeonDB")
    logger.info("  ‚Üí Ejecutar an√°lisis en la capa Gold")
    logger.info("  ‚Üí Generar insights y reportes\n")
    
    return 0


if __name__ == '__main__':
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Pipeline interrumpido por el usuario")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error inesperado: {e}")
        logger.exception("Traceback:")
        sys.exit(1)

